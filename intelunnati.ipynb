{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dc0f98-7467-4ed9-8a2c-8827698b4766",
   "metadata": {},
   "outputs": [],
   "source": [
    "Running GENAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1be348f-e52c-4cb7-a5a9-4f374a054a79",
   "metadata": {},
   "source": [
    "# Installation and Setup\n",
    "\n",
    "This cell installs the required packages and sets up the environment for the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523c5446-6237-45fc-b0e4-59e4f18fa6e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip uninstall -y optimum optimum-intel\n",
    "\n",
    "!pip install --pre -U openvino openvino-tokenizers[transformers] --extra-index-url https://storage.openvinotoolkit.org/simple/wheels/nightly\n",
    "\n",
    "!pip install --extra-index-url https://download.pytorch.org/whl/cpu \\\n",
    "    git+https://github.com/huggingface/optimum-intel.git \\\n",
    "    git+https://github.com/openvinotoolkit/nncf.git \\\n",
    "    torch>=2.1 \\\n",
    "    datasets \\\n",
    "    accelerate \\\n",
    "    gradio>=4.19 \\\n",
    "    onnx \\\n",
    "    einops \\\n",
    "    transformers_stream_generator \\\n",
    "    tiktoken \\\n",
    "    transformers>=4.38.1 \\\n",
    "    bitsandbytes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0a2d55-56ac-4ee0-93f9-d4f051c11fb5",
   "metadata": {},
   "source": [
    "# Model Configuration\n",
    "\n",
    "This cell loads the model configuration and sets up the model ID and language.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5247e9b-7dee-46c9-a8d7-b4b517e63300",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected model neural-chat-7b-v3-1\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "import shutil\n",
    "import os\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "config_shared_path = Path(\"../../utils/llm_config.py\")\n",
    "config_dst_path = Path(\"llm_config.py\")\n",
    "\n",
    "if not config_dst_path.exists():\n",
    "    if config_shared_path.exists():\n",
    "        try:\n",
    "            os.symlink(config_shared_path, config_dst_path)\n",
    "        except Exception:\n",
    "            shutil.copy(config_shared_path, config_dst_path)\n",
    "    else:\n",
    "        r = requests.get(url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/llm_config.py\")\n",
    "        with open(\"llm_config.py\", \"w\") as f:\n",
    "            f.write(r.text)\n",
    "elif not os.path.islink(config_dst_path):\n",
    "    print(\"LLM config will be updated\")\n",
    "    if config_shared_path.exists():\n",
    "        shutil.copy(config_shared_path, config_dst_path)\n",
    "    else:\n",
    "        r = requests.get(url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/llm_config.py\")\n",
    "        with open(\"llm_config.py\", \"w\") as f:\n",
    "            f.write(r.text)\n",
    "\n",
    "from llm_config import SUPPORTED_LLM_MODELS\n",
    "\n",
    "model_language = \"English\"\n",
    "model_id_value = \"neural-chat-7b-v3-1\"\n",
    "\n",
    "model_configuration = SUPPORTED_LLM_MODELS[model_language][model_id_value]\n",
    "print(f\"Selected model {model_id_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53773715-68c4-498b-a7f9-1597390c9306",
   "metadata": {},
   "source": [
    "# Model Preparation\n",
    "\n",
    "This cell prepares the model for conversion to different formats (FP16, INT8, INT4) and displays toggle buttons for model preparation options.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "082b3957-2260-4a21-b1ee-7ee2ec218217",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "782f20c475e5490ba42ca8b812cf36e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButton(value=True, description='Prepare FP16 model')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a8b20302e4a45608ee50f2cabe80aca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButton(value=False, description='Prepare INT8 model')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f68074f447c42bf9bf72e7c22d89a03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButton(value=False, description='Prepare INT4 model')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Directories for different model formats\n",
    "model_dir = Path(model_id_value)\n",
    "fp16_model_dir = model_dir / \"FP16\"\n",
    "int8_model_dir = model_dir / \"INT8_compressed_weights\"\n",
    "int4_model_dir = model_dir / \"INT4_compressed_weights\"\n",
    "\n",
    "# Function to convert model to FP16 format\n",
    "def convert_to_fp16():\n",
    "    if (fp16_model_dir / \"openvino_model.xml\").exists():\n",
    "        return\n",
    "    remote_code = model_configuration.get(\"remote_code\", False)\n",
    "    export_command_base = f\"optimum-cli export openvino --model {model_configuration['model_id']} --task text-generation-with-past --weight-format fp16\"\n",
    "    if remote_code:\n",
    "        export_command_base += \" --trust-remote-code\"\n",
    "    export_command = export_command_base + \" \" + str(fp16_model_dir)\n",
    "    display(Markdown(\"**Export command:**\"))\n",
    "    display(Markdown(f\"`{export_command}`\"))\n",
    "    os.system(export_command)\n",
    "\n",
    "# Function to convert model to INT8 format\n",
    "def convert_to_int8():\n",
    "    if (int8_model_dir / \"openvino_model.xml\").exists():\n",
    "        return\n",
    "    int8_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "    remote_code = model_configuration.get(\"remote_code\", False)\n",
    "    export_command_base = f\"optimum-cli export openvino --model {model_configuration['model_id']} --task text-generation-with-past --weight-format int8\"\n",
    "    if remote_code:\n",
    "        export_command_base += \" --trust-remote-code\"\n",
    "    export_command = export_command_base + \" \" + str(int8_model_dir)\n",
    "    display(Markdown(\"**Export command:**\"))\n",
    "    display(Markdown(f\"`{export_command}`\"))\n",
    "    os.system(export_command)\n",
    "\n",
    "# Function to convert model to INT4 format\n",
    "def convert_to_int4():\n",
    "    compression_configs = {\n",
    "        \"neural-chat-7b-v3-1\": {\"sym\": True, \"group_size\": 64, \"ratio\": 0.6},\n",
    "        \"default\": {\"sym\": False, \"group_size\": 128, \"ratio\": 0.8},\n",
    "    }\n",
    "\n",
    "    model_compression_params = compression_configs.get(model_id_value, compression_configs[\"default\"])\n",
    "    if (int4_model_dir / \"openvino_model.xml\").exists():\n",
    "        return\n",
    "    remote_code = model_configuration.get(\"remote_code\", False)\n",
    "    export_command_base = f\"optimum-cli export openvino --model {model_configuration['model_id']} --task text-generation-with-past --weight-format int4\"\n",
    "    int4_compression_args = f\" --group-size {model_compression_params['group_size']} --ratio {model_compression_params['ratio']}\"\n",
    "    if model_compression_params[\"sym\"]:\n",
    "        int4_compression_args += \" --sym\"\n",
    "    export_command_base += int4_compression_args\n",
    "    if remote_code:\n",
    "        export_command_base += \" --trust-remote-code\"\n",
    "    export_command = export_command_base + \" \" + str(int4_model_dir)\n",
    "    display(Markdown(\"**Export command:**\"))\n",
    "    display(Markdown(f\"`{export_command}`\"))\n",
    "    os.system(export_command)\n",
    "\n",
    "# Toggle buttons for model preparation options\n",
    "prepare_fp16_model = widgets.ToggleButton(\n",
    "    value=True,\n",
    "    description=\"Prepare FP16 model\",\n",
    "    disabled=False,\n",
    ")\n",
    "prepare_int8_model = widgets.ToggleButton(\n",
    "    value=False,\n",
    "    description=\"Prepare INT8 model\",\n",
    "    disabled=False,\n",
    ")\n",
    "prepare_int4_model = widgets.ToggleButton(\n",
    "    value=False,\n",
    "    description=\"Prepare INT4 model\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "# Display toggle buttons\n",
    "display(prepare_fp16_model)\n",
    "display(prepare_int8_model)\n",
    "display(prepare_int4_model)\n",
    "\n",
    "# Function to handle toggling of model preparation options\n",
    "def handle_toggle(change):\n",
    "    if prepare_fp16_model.value:\n",
    "        convert_to_fp16()\n",
    "    if prepare_int8_model.value:\n",
    "        convert_to_int8()\n",
    "    if prepare_int4_model.value:\n",
    "        convert_to_int4()\n",
    "\n",
    "# Attach toggle button event handlers\n",
    "prepare_fp16_model.observe(handle_toggle, names='value')\n",
    "prepare_int8_model.observe(handle_toggle, names='value')\n",
    "prepare_int4_model.observe(handle_toggle, names='value')\n",
    "\n",
    "# Display toggled models\n",
    "handle_toggle(None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f87dcf-be5c-4018-a7e8-25009332626e",
   "metadata": {},
   "source": [
    "# Model Size and Compression Rate\n",
    "\n",
    "This cell calculates and displays the size of the model in different formats and the compression rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2e09fb6-ff6b-4f28-a218-7d420240dea9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of FP16 model is 13828.51 MB\n",
      "Size of model with INT8 compressed weights is 6943.14 MB\n",
      "Compression rate for INT8 model: 1.992\n",
      "Size of model with INT4 compressed weights is 5069.90 MB\n",
      "Compression rate for INT4 model: 2.728\n"
     ]
    }
   ],
   "source": [
    "fp16_weights = fp16_model_dir / \"openvino_model.bin\"\n",
    "int8_weights = int8_model_dir / \"openvino_model.bin\"\n",
    "int4_weights = int4_model_dir / \"openvino_model.bin\"\n",
    "\n",
    "if fp16_weights.exists():\n",
    "    print(f\"Size of FP16 model is {fp16_weights.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "for precision, compressed_weights in zip([8, 4], [int8_weights, int4_weights]):\n",
    "    if compressed_weights.exists():\n",
    "        print(f\"Size of model with INT{precision} compressed weights is {compressed_weights.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "    if compressed_weights.exists() and fp16_weights.exists():\n",
    "        print(f\"Compression rate for INT{precision} model: {fp16_weights.stat().st_size / compressed_weights.stat().st_size:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a040b7d-7f24-449f-ba7d-e6b393dfbdb5",
   "metadata": {},
   "source": [
    "## Device Selection\n",
    "\n",
    "This cell allows the user to select the device (CPU or GPU) for running the inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd758025-92ef-459e-9527-1d116f400a68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0a6c452339a499bb0d356eed7fa9a41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', options=('CPU', 'AUTO'), value='CPU')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d0e5bb81b1a4e30b80439f134622e88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model to run:', options=('INT4', 'INT8', 'FP16'), value='INT4')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import openvino as ov\n",
    "\n",
    "core = ov.Core()\n",
    "\n",
    "support_devices = core.available_devices\n",
    "if \"NPU\" in support_devices:\n",
    "    support_devices.remove(\"NPU\")\n",
    "\n",
    "device = widgets.Dropdown(\n",
    "    options=support_devices + [\"AUTO\"],\n",
    "    value=\"CPU\",\n",
    "    description=\"Device:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "display(device)\n",
    "\n",
    "available_models = []\n",
    "if int4_model_dir.exists():\n",
    "    available_models.append(\"INT4\")\n",
    "if int8_model_dir.exists():\n",
    "    available_models.append(\"INT8\")\n",
    "if fp16_model_dir.exists():\n",
    "    available_models.append(\"FP16\")\n",
    "\n",
    "model_to_run = widgets.Dropdown(\n",
    "    options=available_models,\n",
    "    value=available_models[0],\n",
    "    description=\"Model to run:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "display(model_to_run)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecad16f-05d5-4e06-a8ce-ac06f1c4b8b3",
   "metadata": {},
   "source": [
    "## OpenVINO-based Inference\n",
    "\n",
    "This cell runs the inference using the selected OpenVINO-optimized model, device, and prompt, and displays the output and accuracy score, leveraging OpenVINO's optimized inference engine for efficient and accelerated performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71a87657-4223-4ce2-b941-d3054b24230c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/venv/openvino_notebooks/openvino_2023.3.0_python3.10/.venv/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, tensorflow, onnx, openvino\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-14 05:07:56.415198: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-14 05:07:56.488995: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-14 05:07:56.847066: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-14 05:07:59.992152: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/data/venv/openvino_notebooks/openvino_2023.3.0_python3.10/.venv/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/data/venv/openvino_notebooks/openvino_2023.3.0_python3.10/.venv/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    .custom-dropdown {\n",
       "        width: 300px;\n",
       "        margin: 0 auto;\n",
       "    }\n",
       "    .custom-text {\n",
       "        width: 80%;\n",
       "        margin: 0 auto;\n",
       "    }\n",
       "    .custom-button {\n",
       "        width: 30%;\n",
       "        margin: 20px auto;\n",
       "        font-size: 16px;\n",
       "    }\n",
       "    .output-area {\n",
       "        border: 2px solid #4CAF50;\n",
       "        padding: 20px;\n",
       "        margin: 20px auto;\n",
       "        width: 80%;\n",
       "        background-color: #f9f9f9;\n",
       "        font-family: 'Courier New', Courier, monospace;\n",
       "        font-size: 14px;\n",
       "    }\n",
       "    .output-text {\n",
       "        color: #0000FF;\n",
       "    }\n",
       "    @keyframes fadeIn {\n",
       "        from { opacity: 0; }\n",
       "        to { opacity: 1; }\n",
       "    }\n",
       "    .fade-in {\n",
       "        animation: fadeIn 2s;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f814f5a96c504de8a2c6e3e75f5e419b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(VBox(children=(Dropdown(description='Model Type:', index=1, layout=Layout(width='300px'), optioâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer\n",
    "from optimum.intel.openvino import OVModelForCausalLM\n",
    "from IPython.display import display, HTML\n",
    "import torch\n",
    "import ipywidgets as widgets\n",
    "import contextlib\n",
    "import io\n",
    "import sys\n",
    "\n",
    "# Dummy values for example purposes\n",
    "model_dirs = {\n",
    "    \"INT4\": \"neural-chat-7b-v3-1/INT4_compressed_weights\",\n",
    "    \"INT8\": \"neural-chat-7b-v3-1/INT8_compressed_weights\",\n",
    "    \"FP16\": \"neural-chat-7b-v3-1/FP16\"\n",
    "}\n",
    "\n",
    "# Add CSS for custom styles and animations\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    "    .custom-dropdown {\n",
    "        width: 300px;\n",
    "        margin: 0 auto;\n",
    "    }\n",
    "    .custom-text {\n",
    "        width: 80%;\n",
    "        margin: 0 auto;\n",
    "    }\n",
    "    .custom-button {\n",
    "        width: 30%;\n",
    "        margin: 20px auto;\n",
    "        font-size: 16px;\n",
    "    }\n",
    "    .output-area {\n",
    "        border: 2px solid #4CAF50;\n",
    "        padding: 20px;\n",
    "        margin: 20px auto;\n",
    "        width: 80%;\n",
    "        background-color: #f9f9f9;\n",
    "        font-family: 'Courier New', Courier, monospace;\n",
    "        font-size: 14px;\n",
    "    }\n",
    "    .output-text {\n",
    "        color: #0000FF;\n",
    "    }\n",
    "    @keyframes fadeIn {\n",
    "        from { opacity: 0; }\n",
    "        to { opacity: 1; }\n",
    "    }\n",
    "    .fade-in {\n",
    "        animation: fadeIn 2s;\n",
    "    }\n",
    "</style>\n",
    "\"\"\"))\n",
    "\n",
    "# Widgets for user input\n",
    "model_dropdown = widgets.Dropdown(\n",
    "    options=[\"INT4\", \"INT8\", \"FP16\"],\n",
    "    value=\"INT8\",\n",
    "    description=\"Model Type:\",\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='300px')\n",
    ")\n",
    "\n",
    "device_dropdown = widgets.Dropdown(\n",
    "    options=[\"CPU\", \"GPU\"],\n",
    "    value=\"CPU\",\n",
    "    description=\"Device:\",\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='300px')\n",
    ")\n",
    "\n",
    "prompt_text = widgets.Text(\n",
    "    value=\"What is a constructor?\",\n",
    "    description=\"Prompt:\",\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "\n",
    "run_button = widgets.Button(\n",
    "    description=\"Run Inference\",\n",
    "    button_style='success',\n",
    "    layout=widgets.Layout(width='30%')\n",
    ")\n",
    "\n",
    "output_area = widgets.Output(layout=widgets.Layout(width='80%', padding='20px', border='2px solid #4CAF50', background_color='#f9f9f9'))\n",
    "\n",
    "# Function to suppress stdout and stderr\n",
    "@contextlib.contextmanager\n",
    "def suppress_output():\n",
    "    new_stdout = io.StringIO()\n",
    "    new_stderr = io.StringIO()\n",
    "    old_stdout = sys.stdout\n",
    "    old_stderr = sys.stderr\n",
    "    try:\n",
    "        sys.stdout = new_stdout\n",
    "        sys.stderr = new_stderr\n",
    "        yield\n",
    "    finally:\n",
    "        sys.stdout = old_stdout\n",
    "        sys.stderr = old_stderr\n",
    "\n",
    "# Function to run the inference\n",
    "def run_inference(b):\n",
    "    with output_area:\n",
    "        output_area.clear_output()\n",
    "        model_to_run = model_dropdown.value\n",
    "        device = device_dropdown.value\n",
    "        prompt = prompt_text.value\n",
    "\n",
    "        model_dir = model_dirs.get(model_to_run, \"default/path\")\n",
    "\n",
    "        ov_config = {\"PERFORMANCE_HINT\": \"LATENCY\", \"NUM_STREAMS\": \"1\", \"CACHE_DIR\": \"\"}\n",
    "\n",
    "        # Load tokenizer\n",
    "        try:\n",
    "            with suppress_output():\n",
    "                tok = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "        except Exception as e:\n",
    "            output_area.append_stdout(f\"Error loading tokenizer: {e}\\n\")\n",
    "            return\n",
    "\n",
    "        # Load OVModelForCausalLM\n",
    "        try:\n",
    "            with suppress_output():\n",
    "                ov_model = OVModelForCausalLM.from_pretrained(\n",
    "                    model_dir,\n",
    "                    device=device,\n",
    "                    ov_config=ov_config,\n",
    "                    config=AutoConfig.from_pretrained(model_dir, trust_remote_code=True),\n",
    "                )\n",
    "        except Exception as e:\n",
    "            output_area.append_stdout(f\"Error loading OVModelForCausalLM: {e}\\n\")\n",
    "            return\n",
    "\n",
    "        # Prepare prompt and initial inputs\n",
    "        with suppress_output():\n",
    "            tokens = tok.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "        # Perform inference\n",
    "        try:\n",
    "            with suppress_output():\n",
    "                res = ov_model.generate(\n",
    "                    tokens,\n",
    "                    do_sample=False,\n",
    "                    temperature=0.9,\n",
    "                    repetition_penalty=1.1,\n",
    "                    top_k=1,\n",
    "                    max_new_tokens=100,\n",
    "                )\n",
    "                output = tok.decode(res[0], skip_special_tokens=True)\n",
    "        except Exception as e:\n",
    "            output_area.append_stdout(f\"Error during inference: {e}\\n\")\n",
    "            return\n",
    "\n",
    "        # Display output\n",
    "        output_area.append_stdout(f\"Output:\\n\")\n",
    "        output_area.append_display_data(HTML(f'<div class=\"fade-in output-text\">{output}</div>'))\n",
    "\n",
    "        # Accuracy check (example)\n",
    "        expected_keywords = [\"constructor\", \"class\", \"object\"]\n",
    "        accuracy_score = sum([1 for kw in expected_keywords if kw in output.lower()]) / len(expected_keywords)\n",
    "        output_area.append_display_data(HTML(f'<div class=\"fade-in\">Accuracy score: {accuracy_score:.2f}</div>'))\n",
    "\n",
    "run_button.on_click(run_inference)\n",
    "\n",
    "# Organize the layout\n",
    "input_widgets = widgets.VBox([model_dropdown, device_dropdown, prompt_text, run_button], layout=widgets.Layout(align_items='center'))\n",
    "main_layout = widgets.VBox([input_widgets, output_area], layout=widgets.Layout(align_items='center'))\n",
    "\n",
    "# Display the main layout\n",
    "display(main_layout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21683bae-a4cc-439f-a5f0-1fd06df82312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5b1fcb-7b8d-4360-afe5-6aa718a27bb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1fbd5d-4071-47c9-880f-ef85ecebc505",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b9aec7-4ece-4dc9-a70d-a68b743ca8fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d905fe-8dd1-4808-8f1b-4e2cbf0cc394",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a92ca9-00ca-4a4a-88ae-c809aa61eb99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcd148f-6cc4-4ac1-88a1-07653df8f93a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a9cfcfa-8bc6-48a0-ab00-a6de29a98c92",
   "metadata": {},
   "source": [
    "# FINE TUNING META-LLAMA-2-7b-hf using Openvino"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f6676e-de8d-4a74-ae17-0ddb0f1fcfd5",
   "metadata": {},
   "source": [
    "# Fine-Tuning a Language Model with Hugging Face Transformers and Trainer API:\n",
    "\n",
    "Defines model and dataset details.\n",
    "Loads a portion of the \"wikitext\" dataset using Hugging Face datasets library.\n",
    "Sets up training arguments and initializes Trainer for fine-tuning.\n",
    "Trains the model, saves it, and exports it to ONNX format for deployment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c67e995b-9263-4a30-b8d5-aeaa9040dd57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset columns: ['text']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94a33d5d3c734b7281c6d49636d716b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/venv/openvino_notebooks/openvino_2023.3.0_python3.10/.venv/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='92' max='92' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [92/92 44:41, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.732400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.407600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.447400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.141600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.460500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.238300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.342500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.744400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.147300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning completed within the time limit.\n"
     ]
    },
    {
     "ename": "OverflowError",
     "evalue": "cannot fit 'int' into an index-sized integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 81\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Export to ONNX\u001b[39;00m\n\u001b[1;32m     80\u001b[0m onnx_model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./llama\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfine_tuned_model.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 81\u001b[0m dummy_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_max_length\u001b[49m])\n\u001b[1;32m     82\u001b[0m torch\u001b[38;5;241m.\u001b[39monnx\u001b[38;5;241m.\u001b[39mexport(model, dummy_input, onnx_model_path, input_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m], output_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m], opset_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m11\u001b[39m)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel exported to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00monnx_model_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mOverflowError\u001b[0m: cannot fit 'int' into an index-sized integer"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Define model and dataset details\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "dataset_name = \"wikitext\"\n",
    "subset_name = \"wikitext-2-raw-v1\"\n",
    "text_column = \"text\"\n",
    "\n",
    "# Load a smaller portion of the dataset\n",
    "dataset = load_dataset(dataset_name, subset_name, split='train[:1%]')\n",
    "\n",
    "# Check column names\n",
    "print(\"Dataset columns:\", dataset.column_names)\n",
    "\n",
    "# Set the Hugging Face token as an environment variable\n",
    "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = \"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\"\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=True)\n",
    "except OSError as e:\n",
    "    print(f\"Error loading model {model_name}: {e}\")\n",
    "    raise\n",
    "\n",
    "# Add a padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Define the tokenize function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[text_column], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[text_column])\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./new\",\n",
    "    per_device_train_batch_size=4,  # Reduce batch size\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=1,  # Set to 1 epoch for quick testing\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./old\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=False,  # Set to False since evaluation is not performed\n",
    ")\n",
    "\n",
    "# Define the data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"./llama\")\n",
    "tokenizer.save_pretrained(\"./llama\")\n",
    "\n",
    "print(\"Fine-tuning completed within the time limit.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6955dcd0-7a55-4598-8e17-cc1713e1dc69",
   "metadata": {},
   "source": [
    "## Wrapping Fine-Tuned Model for ONNX Export:\n",
    "\n",
    "Defines a custom model wrapper class to handle input and output formats.\n",
    "Loads the fine-tuned Llama model using Hugging Face Transformers.\n",
    "Wraps the model with the custom wrapper for compatibility with ONNX export.\n",
    "Exports the wrapped model to ONNX format using opset version 14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3996b1b4-2de8-4234-beb0-6e9f5bee6c38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3e6dacc7a204dc3a1bc03df7c392d4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/master/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/master/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/master/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/master/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/master/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/master/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/master/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/master/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/master/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/master/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/master/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/master/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/master/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/master/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/master/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/master/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/master/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/master/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/master/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/master/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/master/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/master/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/master/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/master/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/master/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/master/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/master/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/master/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/master/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/master/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/master/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/master/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model exported to ./llama/fine_tuned_model.onnx.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.onnx\n",
    "import transformers\n",
    "import os\n",
    "\n",
    "class ModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(ModelWrapper, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        return self.model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "\n",
    "# Load your fine-tuned Llama model\n",
    "model_path = \"./llama/fine_tuned_model\"\n",
    "model = transformers.LlamaForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "# Wrap the model with the custom wrapper\n",
    "model_wrapper = ModelWrapper(model)\n",
    "\n",
    "# Dummy input for the export\n",
    "dummy_input_ids = torch.zeros((1, 10), dtype=torch.long)\n",
    "dummy_attention_mask = torch.ones((1, 10), dtype=torch.long)\n",
    "\n",
    "# Export to ONNX using opset version 14\n",
    "onnx_model_path = os.path.join(\"./llama\", \"fine_tuned_model.onnx\")\n",
    "torch.onnx.export(model_wrapper, (dummy_input_ids, dummy_attention_mask), onnx_model_path, \n",
    "                  input_names=[\"input_ids\", \"attention_mask\"], output_names=[\"logits\"], opset_version=14)\n",
    "print(f\"Model exported to {onnx_model_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc5b82c-2d86-4099-aff8-777777a316de",
   "metadata": {},
   "source": [
    "## Converting ONNX Model to OpenVINO IR and Performing Inference:\n",
    "\n",
    "Loads the tokenizer for the fine-tuned Llama model.\n",
    "Converts the exported ONNX model to OpenVINO Intermediate Representation (IR) format using Model Optimizer.\n",
    "Loads the optimized model into the Inference Engine (IE) of OpenVINO.\n",
    "Defines a prompt for text generation, encodes it using the tokenizer, and prepares input for OpenVINO.\n",
    "Performs inference using OpenVINO and benchmarks the inference time and throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e0ad012-73a8-469a-a9f7-b69cc17c7b91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ INFO ] Generated IR will be compressed to FP16. If you get lower accuracy, please consider disabling compression explicitly by adding argument --compress_to_fp16=False.\n",
      "Find more information about compression to FP16 at https://docs.openvino.ai/2023.0/openvino_docs_MO_DG_FP16_Compression.html\n",
      "[ INFO ] The model was converted to IR v11, the latest model format that corresponds to the source DL framework input/output format. While IR v11 is backwards compatible with OpenVINO Inference Engine API v1.0, please use API v2.0 (as of 2022.1) to take advantage of the latest improvements in IR v11.\n",
      "Find more information about API v2.0 and IR v11 at https://docs.openvino.ai/2023.0/openvino_2_0_transition_guide.html\n",
      "[ INFO ] MO command line tool is considered as the legacy conversion API as of OpenVINO 2023.2 release. Please use OpenVINO Model Converter (OVC). OVC represents a lightweight alternative of MO and provides simplified model conversion API. \n",
      "Find more information about transition from MO to OVC at https://docs.openvino.ai/2023.2/openvino_docs_OV_Converter_UG_prepare_model_convert_model_MO_OVC_transition.html\n",
      "[ SUCCESS ] Generated IR version 11 model.\n",
      "[ SUCCESS ] XML file: /home/u1031703/Reference-samples/openvino_notebooks/notebooks/llm-chatbot/llama/optimized_model/fine_tuned_model.xml\n",
      "[ SUCCESS ] BIN file: /home/u1031703/Reference-samples/openvino_notebooks/notebooks/llm-chatbot/llama/optimized_model/fine_tuned_model.bin\n",
      "Generated text: to the ;\n",
      "Generated text: to the ;\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from openvino.inference_engine import IECore\n",
    "import subprocess\n",
    "\n",
    "# Load the tokenizer\n",
    "model_path = \"./llama\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Convert the model to OpenVINO IR format\n",
    "onnx_model_path = os.path.join(model_path, \"fine_tuned_model.onnx\")\n",
    "model_path = os.path.join(model_path, \"optimized_model\")\n",
    "mo_command = f\"python -m openvino.tools.mo --input_model {onnx_model_path} --output_dir {model_path} --static_shape\"\n",
    "subprocess.run(mo_command.split())\n",
    "\n",
    "# Load the optimized model\n",
    "ie = IECore()\n",
    "\n",
    "# Adjust CPU configuration for inference\n",
    "ie.set_config({'CPU_THROUGHPUT_STREAMS': '1', 'CPU_BIND_THREAD': 'YES'}, 'CPU')\n",
    "\n",
    "# Load the network\n",
    "model_xml_path = os.path.join(model_path, \"fine_tuned_model.xml\")\n",
    "model_bin_path = os.path.join(model_path, \"fine_tuned_model.bin\")\n",
    "net = ie.read_network(model=model_xml_path, weights=model_bin_path)\n",
    "\n",
    "# Reduce batch size if necessary (example, change 1 to a smaller number)\n",
    "net.batch_size = 1\n",
    "\n",
    "# Load the network to the device (CPU in this case)\n",
    "exec_net = ie.load_network(network=net, device_name=\"CPU\")\n",
    "\n",
    "# Define the prompt for text generation\n",
    "prompt = \"go to market\"\n",
    "\n",
    "# Encode the prompt\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", max_length=10, padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Prepare the input for OpenVINO\n",
    "input_dict = {\"input_ids\": input_ids.cpu().numpy().astype(np.int32)}\n",
    "\n",
    "# Perform inference\n",
    "output = exec_net.infer(inputs=input_dict)\n",
    "\n",
    "# Convert logits to probabilities and sample\n",
    "logits = output[\"logits\"]\n",
    "probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1)\n",
    "generated_ids = torch.argmax(probs, dim=-1)\n",
    "\n",
    "# Decode and print the generated text\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "generated_text = generated_text.strip()  # Remove leading/trailing spaces\n",
    "generated_text = generated_text.replace(\"[PAD]\", \"\")  # Remove padding tokens if any\n",
    "\n",
    "print(\"Generated text:\", generated_text)\n",
    "\n",
    "# Save the generated text to a file\n",
    "output_dir = \"llama/generated\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "with open(os.path.join(output_dir, \"generated_text.txt\"), \"w\") as file:\n",
    "    file.write(generated_text)\n",
    "\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "generated_text = generated_text.strip()  # Remove leading/trailing spaces\n",
    "generated_text = generated_text.replace(\"[PAD]\", \"\")  # Remove padding tokens if any\n",
    "\n",
    "print(\"Generated text:\", generated_text)\n",
    "\n",
    "# Save the generated text to a file\n",
    "output_dir = \"llama/generated\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "with open(os.path.join(output_dir, \"generated_text.txt\"), \"w\") as file:\n",
    "    file.write(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03c638e-7879-4e09-a73c-c784860d743a",
   "metadata": {},
   "source": [
    "## Benchmarking Inference Performance with OpenVINO:\n",
    "\n",
    "Loads the fine-tuned Llama model and tokenizer.\n",
    "Defines an input prompt, encodes it using the tokenizer, and prepares input for OpenVINO.\n",
    "Converts the ONNX model to OpenVINO IR format using Model Optimizer.\n",
    "Loads the optimized model into the OpenVINO Inference Engine (IE) and sets CPU as the device.\n",
    "Performs benchmarking to measure inference time and throughput over 100 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dd39c21-6076-479e-8a65-dc1185464f6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ INFO ] Generated IR will be compressed to FP16. If you get lower accuracy, please consider disabling compression explicitly by adding argument --compress_to_fp16=False.\n",
      "Find more information about compression to FP16 at https://docs.openvino.ai/2023.0/openvino_docs_MO_DG_FP16_Compression.html\n",
      "[ INFO ] The model was converted to IR v11, the latest model format that corresponds to the source DL framework input/output format. While IR v11 is backwards compatible with OpenVINO Inference Engine API v1.0, please use API v2.0 (as of 2022.1) to take advantage of the latest improvements in IR v11.\n",
      "Find more information about API v2.0 and IR v11 at https://docs.openvino.ai/2023.0/openvino_2_0_transition_guide.html\n",
      "[ INFO ] MO command line tool is considered as the legacy conversion API as of OpenVINO 2023.2 release. Please use OpenVINO Model Converter (OVC). OVC represents a lightweight alternative of MO and provides simplified model conversion API. \n",
      "Find more information about transition from MO to OVC at https://docs.openvino.ai/2023.2/openvino_docs_OV_Converter_UG_prepare_model_convert_model_MO_OVC_transition.html\n",
      "[ SUCCESS ] Generated IR version 11 model.\n",
      "[ SUCCESS ] XML file: /home/u1031703/Reference-samples/openvino_notebooks/notebooks/llm-chatbot/llama/fine_tuned_model.xml\n",
      "[ SUCCESS ] BIN file: /home/u1031703/Reference-samples/openvino_notebooks/notebooks/llm-chatbot/llama/fine_tuned_model.bin\n",
      "Inference time: 0.8281 seconds\n",
      "Throughput: 1.21 sequences per second\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from openvino.inference_engine import IECore\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model_path = \"./llama\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Define the input prompt\n",
    "prompt = \"what is constructor\"\n",
    "\n",
    "# Encode the prompt\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", max_length=10, padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Convert the model to OpenVINO IR format\n",
    "onnx_model_path = os.path.join(model_path, \"fine_tuned_model.onnx\")\n",
    "model_xml_path = os.path.join(model_path, \"fine_tuned_model.xml\")\n",
    "model_bin_path = os.path.join(model_path, \"fine_tuned_model.bin\")\n",
    "mo_command = f\"python -m openvino.tools.mo --input_model {onnx_model_path} --output_dir {model_path} --static_shape\"\n",
    "os.system(mo_command)\n",
    "\n",
    "# Load the optimized model\n",
    "ie = IECore()\n",
    "net = ie.read_network(model=model_xml_path, weights=model_bin_path)\n",
    "\n",
    "# Set the device (CPU)\n",
    "exec_net = ie.load_network(network=net, device_name=\"CPU\")\n",
    "\n",
    "# Prepare the input for OpenVINO\n",
    "input_dict = {\"input_ids\": input_ids.cpu().numpy().astype(np.int32)}\n",
    "\n",
    "# Benchmarking\n",
    "batch_size = 1\n",
    "num_iterations = 100\n",
    "\n",
    "start_time = time.time()\n",
    "for _ in range(num_iterations):\n",
    "    output = exec_net.infer(inputs=input_dict)\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the inference time\n",
    "inference_time = (end_time - start_time) / num_iterations\n",
    "print(f\"Inference time: {inference_time:.4f} seconds\")\n",
    "\n",
    "# Calculate the throughput (sequences per second)\n",
    "throughput = batch_size / inference_time\n",
    "print(f\"Throughput: {throughput:.2f} sequences per second\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (OpenVINO Notebooks 2023.3.0)",
   "language": "python",
   "name": "openvino_notebooks_2023.3.0_python3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
